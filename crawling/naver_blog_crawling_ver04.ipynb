{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 순서\n",
    "# 1차 크롤링 : 블로그 조회 -> title / url / date\n",
    "# 2차 크롤링 : url별 방문하여 해시태그 가져오기\n",
    "# 2차 크롤링 기준 추가 : 수집된 hashtag가 200개이면 다음 크롤링 진행 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- url list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../Data/서울시115장소명 목록_장소명수정_20240527.xlsx')\n",
    "keywords = [i for i in df['AREA_SEARCH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = timer()\n",
    "\n",
    "# 검색어 입력\n",
    "df = pd.read_excel('../Data/서울시115장소명 목록_장소명수정_20240527.xlsx')\n",
    "keywords =df['AREA_SEARCH']\n",
    "\n",
    "keywords_data = []\n",
    "\n",
    "# 조회 기간 설정 : 7일(days에서 수정 가능)\n",
    "# startDate=7일전 날짜, endDate=오늘 날짜\n",
    "date = datetime.now()\n",
    "startDate= (date+timedelta(days=-1)).strftime('%Y-%m-%d')\n",
    "endDate = (datetime.now()).strftime('%Y-%m-%d')\n",
    "\n",
    "# 브라우저 옵션 설정\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_argument('headless')\n",
    "options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36')\n",
    "# 브라우저 초기화\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "crawling_blog_data = []\n",
    "for query in keywords:\n",
    "    # start_crawling_time = timer()\n",
    "    # print(start_crawling_time)\n",
    "    try:\n",
    "        # 검색 URL 생성\n",
    "        base_url = f\"https://section.blog.naver.com/Search/Post.naver?pageNo=1&rangeType=WEEK&orderBy=sim&startDate={startDate}&endDate={endDate}&keyword={query}\"\n",
    "        driver.get(base_url)\n",
    "        time.sleep(0.5)  # 페이지가 로드될 때까지 대기\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        # search_number = soup.find(class_=\"search_number\")\n",
    "        # total_num = search_number.text.replace(\"건\",'').replace(\",\",'')\n",
    "        # total_num = int(total_num)\n",
    "        # end_page=total_num//7+1\n",
    "\n",
    "        # 페이지마다 게시물 가져오기\n",
    "        for page_num in range(1, 100):\n",
    "            search_url = f\"https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType=WEEK&orderBy=sim&startDate={startDate}&endDate={endDate}&keyword={query}\"\n",
    "            driver.get(search_url)\n",
    "            time.sleep(0.5)  # 페이지가 로드될 때까지 대기\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            # 블로그 포스트 데이터 가져오기\n",
    "            posts_data = soup.select('div.info_post > div.desc > a.desc_inner')\n",
    "            posts_date_data = soup.select('div.info_post > div.writer_info')\n",
    "\n",
    "            # 타이틀 수가 7개가 아니면 루프를 빠져나오기\n",
    "            if len(posts_data) != 1:\n",
    "                for post, post_date in zip(posts_data, posts_date_data):\n",
    "                    title = post.text.replace('\\n', '').strip()\n",
    "                    href = post.attrs['href']\n",
    "                    date = post_date.find('span', 'date').text.strip()\n",
    "                    try :\n",
    "                        date_timestamp = datetime.strptime(date, '%Y. %m. %d.').strftime('%Y-%m-%d')\n",
    "                    except:\n",
    "                        date_timestamp = endDate\n",
    "                    crawling_blog_data.append({\"location\": query, \"title\": title, \"url\": href, \"posted_date\": date_timestamp})\n",
    "            else :\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"에러 발생: {e}\")\n",
    "\n",
    "    # end_crawling_time = timer()\n",
    "    # print(end_crawling_time)\n",
    "    # print(\"working_crawling_time: {} sec\".format(end_crawling_time-start_crawling_time))\n",
    "\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()\n",
    "# end_time = timer()\n",
    "# print(\"working_time: {} sec\".format(end_time-start_time))\n",
    "\n",
    "\n",
    "crawling_blog_data = pd.DataFrame(crawling_blog_data)\n",
    "file_name = f'../Data/crawling_blog_list_{startDate}-{endDate}.csv'\n",
    "crawling_blog_data.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- hashtag list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url list 가져오기\n",
    "crawling_data = pd.read_csv(file_name)\n",
    "crawling_data\n",
    "\n",
    "# location list\n",
    "df = pd.read_excel('../Data/서울시115장소명 목록_장소명수정_20240527.xlsx')\n",
    "test_df=[location for location in df['AREA_SEARCH']]\n",
    "\n",
    "# Chrome driver 실행\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome(options = options)\n",
    "\n",
    "crawling_contents_data = []\n",
    "\n",
    "for keyword in keywords:\n",
    "    temp = crawling_data[crawling_data['location'] == keyword]\n",
    "    total_tags=0\n",
    "    for index, row in temp.iterrows():\n",
    "        url = str(row['url'])\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            # iframe 접근\n",
    "            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.ID, 'mainFrame')))\n",
    "            # tag 찾아서 가져오기\n",
    "            tags_element = driver.find_element(By.CLASS_NAME, \"wrap_tag\")\n",
    "            tags = tags_element.text.strip('태그').split('\\n')\n",
    "            tags=[tag for tag in tags if tag != '']\n",
    "            tags = [tag.replace(keyword,\"\") for tag in tags if tag != '']\n",
    "            tags = [tag.replace(keyword,\"\") for tag in tags if tag != '#']\n",
    "\n",
    "            # 제외할 단어 리스트 (추가 예정)\n",
    "            # exclud_list =[]\n",
    "            # if tags in exclude_list:\n",
    "            #   continue\n",
    "\n",
    "            # data 담기\n",
    "            if len(tags)>0:\n",
    "                crawling_contents_data.append({\"location\": keyword, \"url\": url, \"tags\": tags})\n",
    "                total_tags += len(tags)\n",
    "                print(f'{index}번째 완료: {tags} / 총 {total_tags} 개')\n",
    "                # 해시태그 개수가 200개 이상이면 다음 키워드로 넘어가기\n",
    "                if total_tags >= 200:\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error ({index}) : {str(e)}\")\n",
    "\n",
    "    # 해시태그 개수가 200개 이상이면 다음 키워드로 넘어가기\n",
    "    # if total_tags >= 200:\n",
    "    #     break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 파일저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = pd.DataFrame(crawling_contents_data)\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'../Data/crawling_tags_data_{startDate}-{endDate}.csv'\n",
    "df_tags.to_csv(f\"{file_name}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
